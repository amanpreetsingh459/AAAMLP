{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic regression using one-hot-encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"../input/adult_folds.csv\")\n",
    "    \n",
    "    # list of numerical columns\n",
    "    num_cols = [\n",
    "                \"fnlwgt\",\n",
    "                \"age\",\n",
    "                \"capital.gain\",\n",
    "                \"capital.loss\",\n",
    "                \"hours.per.week\"\n",
    "                ]\n",
    "    \n",
    "    # drop numerical columns\n",
    "    df = df.drop(num_cols, axis=1)\n",
    "    \n",
    "    # map targets to 0s and 1s\n",
    "    target_mapping = {\n",
    "                        \"<=50K\": 0,\n",
    "                        \">50K\": 1\n",
    "                        }\n",
    "    \n",
    "    df.loc[:, \"income\"] = df.income.map(target_mapping)\n",
    "    \n",
    "    # all columns are features except income and kfold columns\n",
    "    features = [\n",
    "                    f for f in df.columns if f not in (\"kfold\", \"income\")\n",
    "                ]\n",
    "    # fill all NaN values with NONE\n",
    "    # note that I am converting all columns to \"strings\"\n",
    "    # it doesnt matter because all are categories\n",
    "    for col in features:\n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "        \n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    \n",
    "    # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "    \n",
    "    # initialize OneHotEncoder from scikit-learn\n",
    "    ohe = preprocessing.OneHotEncoder()\n",
    "    \n",
    "    # fit ohe on training + validation features\n",
    "    full_data = pd.concat(\n",
    "                    [df_train[features], df_valid[features]],\n",
    "                    axis=0\n",
    "                )\n",
    "    \n",
    "    ohe.fit(full_data[features])\n",
    "    \n",
    "    # transform training data\n",
    "    x_train = ohe.transform(df_train[features])\n",
    "    \n",
    "    # transform validation data\n",
    "    x_valid = ohe.transform(df_valid[features])\n",
    "    \n",
    "    # initialize Logistic Regression model\n",
    "    model = linear_model.LogisticRegression()\n",
    "    \n",
    "    # fit model on training data (ohe)\n",
    "    model.fit(x_train, df_train.income.values)\n",
    "    \n",
    "    # predict on validation data\n",
    "    # we need the probability values as we are calculating AUC\n",
    "    # we will use the probability of 1s\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    \n",
    "    # get roc auc score\n",
    "    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)\n",
    "    # print auc\n",
    "    print(f\"Fold = {fold}, AUC = {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Miniconda3\\envs\\aaamlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold = 0, AUC = 0.8794834201695059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Miniconda3\\envs\\aaamlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold = 1, AUC = 0.8876246873142462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Miniconda3\\envs\\aaamlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold = 2, AUC = 0.8852609687685753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Miniconda3\\envs\\aaamlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold = 3, AUC = 0.8681225903589591\n",
      "Fold = 4, AUC = 0.8728581541840037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Miniconda3\\envs\\aaamlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "for fold_ in range(5):\n",
    "    run(fold_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost with label-encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"../input/adult_folds.csv\")\n",
    "    \n",
    "    # list of numerical columns\n",
    "    num_cols = [\n",
    "                \"fnlwgt\",\n",
    "                \"age\",\n",
    "                \"capital.gain\",\n",
    "                \"capital.loss\",\n",
    "                \"hours.per.week\"\n",
    "                ]\n",
    "    \n",
    "    # drop numerical columns\n",
    "    df = df.drop(num_cols, axis=1)\n",
    "    \n",
    "    # map targets to 0s and 1s\n",
    "    target_mapping = {\n",
    "                        \"<=50K\": 0,\n",
    "                        \">50K\": 1\n",
    "                        }\n",
    "    \n",
    "    df.loc[:, \"income\"] = df.income.map(target_mapping)\n",
    "    \n",
    "    # all columns are features except kfold & income columns\n",
    "    features = [\n",
    "                    f for f in df.columns if f not in (\"kfold\", \"income\")\n",
    "                ]\n",
    "    \n",
    "    # fill all NaN values with NONE\n",
    "    # note that I am converting all columns to \"strings\"\n",
    "    # it doesnt matter because all are categories\n",
    "    for col in features:\n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "        \n",
    "    # now its time to label encode the features\n",
    "    for col in features:\n",
    "        # initialize LabelEncoder for each feature column\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        \n",
    "        # fit label encoder on all data\n",
    "        lbl.fit(df[col])\n",
    "        \n",
    "        # transform all the data\n",
    "        df.loc[:, col] = lbl.transform(df[col])\n",
    "        \n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    \n",
    "    # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "    \n",
    "    # get training data\n",
    "    x_train = df_train[features].values\n",
    "    \n",
    "    # get validation data\n",
    "    x_valid = df_valid[features].values\n",
    "    \n",
    "    # initialize xgboost model\n",
    "    model = xgb.XGBClassifier(\n",
    "                                n_jobs=-1,\n",
    "                                max_depth=7,\n",
    "                                n_estimators=200\n",
    "                            )\n",
    "    \n",
    "    # fit model on training data (ohe)\n",
    "    model.fit(x_train, df_train.income.values)\n",
    "    \n",
    "    # predict on validation data\n",
    "    # we need the probability values as we are calculating AUC\n",
    "    # we will use the probability of 1s\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    \n",
    "    # get roc auc score\n",
    "    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)\n",
    "    # print auc\n",
    "    print(f\"Fold = {fold}, AUC = {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold = 0, AUC = 0.8764108944332032\n",
      "Fold = 1, AUC = 0.8848888159632786\n",
      "Fold = 2, AUC = 0.8816601162613102\n",
      "Fold = 3, AUC = 0.8662335762581732\n",
      "Fold = 4, AUC = 0.8698983461709927\n"
     ]
    }
   ],
   "source": [
    "for fold_ in range(5):\n",
    "    run(fold_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost model with numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"../input/adult_folds.csv\")\n",
    "    \n",
    "    # list of numerical columns\n",
    "    num_cols = [\n",
    "                \"fnlwgt\",\n",
    "                \"age\",\n",
    "                \"capital.gain\",\n",
    "                \"capital.loss\",\n",
    "                \"hours.per.week\"\n",
    "                ]\n",
    "    \n",
    "    # map targets to 0s and 1s\n",
    "    target_mapping = {\n",
    "                \"<=50K\": 0,\n",
    "                \">50K\": 1\n",
    "                }\n",
    "    \n",
    "    df.loc[:, \"income\"] = df.income.map(target_mapping)\n",
    "    \n",
    "    # all columns are features except kfold & income columns\n",
    "    features = [\n",
    "                    f for f in df.columns if f not in (\"kfold\", \"income\")\n",
    "                ]\n",
    "    \n",
    "    # fill all NaN values with NONE\n",
    "    # note that I am converting all columns to \"strings\"\n",
    "    # it doesnt matter because all are categories\n",
    "    for col in features:\n",
    "        # do not encode the numerical columns\n",
    "        if col not in num_cols:\n",
    "            df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "            \n",
    "    # now its time to label encode the features\n",
    "    for col in features:\n",
    "        if col not in num_cols:\n",
    "            # initialize LabelEncoder for each feature column\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            \n",
    "            # fit label encoder on all data\n",
    "            lbl.fit(df[col])\n",
    "            \n",
    "            # transform all the data\n",
    "            df.loc[:, col] = lbl.transform(df[col])\n",
    "            \n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    \n",
    "    # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "    \n",
    "    # get training data\n",
    "    x_train = df_train[features].values\n",
    "    \n",
    "    # get validation data\n",
    "    x_valid = df_valid[features].values\n",
    "    \n",
    "    # initialize xgboost model\n",
    "    model = xgb.XGBClassifier(\n",
    "                            n_jobs=-1\n",
    "                            )\n",
    "    \n",
    "    # fit model on training data (ohe)\n",
    "    model.fit(x_train, df_train.income.values)\n",
    "    \n",
    "    # predict on validation data\n",
    "    # we need the probability values as we are calculating AUC\n",
    "    # we will use the probability of 1s\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    \n",
    "    # get roc auc score\n",
    "    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)\n",
    "    \n",
    "    # print auc\n",
    "    print(f\"Fold = {fold}, AUC = {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold = 0, AUC = 0.9209790185449889\n",
      "Fold = 1, AUC = 0.9247157449144706\n",
      "Fold = 2, AUC = 0.9269329887598243\n",
      "Fold = 3, AUC = 0.9119349082169275\n",
      "Fold = 4, AUC = 0.9166408030141667\n"
     ]
    }
   ],
   "source": [
    "for fold_ in range(5):\n",
    "    run(fold_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take all the categorical columns and create all combinations of degree two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df, cat_cols):\n",
    "    \"\"\"\n",
    "    This function is used for feature engineering\n",
    "    :param df: the pandas dataframe with train/test data\n",
    "    :param cat_cols: list of categorical columns\n",
    "    :return: dataframe with new features\n",
    "    \"\"\"\n",
    "    \n",
    "    # this will create all 2-combinations of values\n",
    "    # in this list\n",
    "    # for example:\n",
    "    # list(itertools.combinations([1,2,3], 2)) will return\n",
    "    # [(1, 2), (1, 3), (2, 3)]\n",
    "    combi = list(itertools.combinations(cat_cols, 2))\n",
    "    \n",
    "    for c1, c2 in combi:\n",
    "        df.loc[\n",
    "        :,\n",
    "        c1 + \"_\" + c2\n",
    "        ] = df[c1].astype(str) + \"_\" + df[c2].astype(str)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"../input/adult_folds.csv\")\n",
    "    \n",
    "    # list of numerical columns\n",
    "    num_cols = [\n",
    "                \"fnlwgt\",\n",
    "                \"age\",\n",
    "                \"capital.gain\",\n",
    "                \"capital.loss\",\n",
    "                \"hours.per.week\"\n",
    "                ]\n",
    "    \n",
    "    # map targets to 0s and 1s\n",
    "    target_mapping = {\n",
    "                \"<=50K\": 0,\n",
    "                \">50K\": 1\n",
    "                }\n",
    "    \n",
    "    df.loc[:, \"income\"] = df.income.map(target_mapping)\n",
    "    \n",
    "    # list of categorical columns for feature engineering\n",
    "    cat_cols = [\n",
    "                c for c in df.columns if c not in num_cols\n",
    "                and c not in (\"kfold\", \"income\")\n",
    "                ]\n",
    "    \n",
    "    # add new features\n",
    "    df = feature_engineering(df, cat_cols)\n",
    "    \n",
    "    # all columns are features except kfold & income columns\n",
    "    features = [\n",
    "                f for f in df.columns if f not in (\"kfold\", \"income\")\n",
    "                ]\n",
    "    \n",
    "    # fill all NaN values with NONE\n",
    "    # note that I am converting all columns to \"strings\"\n",
    "    # it doesnt matter because all are categories\n",
    "    for col in features:\n",
    "        # do not encode the numerical columns\n",
    "        if col not in num_cols:\n",
    "            df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "\n",
    "    # now its time to label encode the features\n",
    "    for col in features:\n",
    "        if col not in num_cols:\n",
    "            # initialize LabelEncoder for each feature column\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            \n",
    "            # fit label encoder on all data\n",
    "            lbl.fit(df[col])\n",
    "            \n",
    "            # transform all the data\n",
    "            df.loc[:, col] = lbl.transform(df[col])\n",
    "            \n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    \n",
    "    # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "    \n",
    "    # get training data\n",
    "    x_train = df_train[features].values\n",
    "    \n",
    "    # get validation data\n",
    "    x_valid = df_valid[features].values\n",
    "    \n",
    "    # initialize xgboost model\n",
    "    model = xgb.XGBClassifier(\n",
    "                n_jobs=-1,\n",
    "                max_depth=7\n",
    "                )\n",
    "    \n",
    "    # fit model on training data (ohe)\n",
    "    model.fit(x_train, df_train.income.values)\n",
    "    \n",
    "    # predict on validation data\n",
    "    # we need the probability values as we are calculating AUC\n",
    "    # we will use the probability of 1s\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    \n",
    "    # get roc auc score\n",
    "    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)\n",
    "    \n",
    "    # print auc\n",
    "    print(f\"Fold = {fold}, AUC = {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold = 0, AUC = 0.9286668430204137\n",
      "Fold = 1, AUC = 0.9329340656165378\n",
      "Fold = 2, AUC = 0.9319817543218744\n",
      "Fold = 3, AUC = 0.919046187194538\n",
      "Fold = 4, AUC = 0.9245692057162671\n"
     ]
    }
   ],
   "source": [
    "for fold_ in range(5):\n",
    "    run(fold_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
